{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell jupyter where pyspark is\n",
    "import findspark\n",
    "findspark.init()\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful stuff\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Models and support\n",
    "from pyspark.sql.functions import col, avg\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a SparkSession; \n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Final Project\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed dataset \n",
    "df = spark.read.csv(\"data/std.csv\",inferSchema =True,header=True)\n",
    "df = df.drop('_c0')\n",
    "n_features = len(df.columns) - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature vector from the data\n",
    "ignore = ['loan_status']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in df.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "df = assembler.transform(df)\n",
    "df = df.select(['loan_status','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first split into a test and training set\n",
    "(trainingData, testData) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Now let's create all our models (Logistic Regression, GBT, Linear SVC, and MLP)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8,\\\n",
    "                        labelCol = 'loan_status', featuresCol = 'features')\n",
    "\n",
    "gbt = GBTClassifier(maxIter=10,\\\n",
    "                    labelCol = 'loan_status', featuresCol = 'features')\n",
    "\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1,\\\n",
    "                 labelCol = 'loan_status', featuresCol = 'features')\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[n_features, 5, 4, 2], blockSize=128,seed=1234,\\\n",
    "                                     labelCol = 'loan_status', featuresCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression training complete\n",
      "Gradient Boosted training complete\n",
      "Linear SVC training complete\n",
      "Multi-layer Perceptron training complete\n"
     ]
    }
   ],
   "source": [
    "# Train all the models\n",
    "lr_model = lr.fit(trainingData)\n",
    "print('Logistic Regression training complete')\n",
    "\n",
    "gbt_model = gbt.fit(trainingData)\n",
    "print('Gradient Boosted training complete')\n",
    "\n",
    "lsvc_model = lsvc.fit(trainingData)\n",
    "print('Linear SVC training complete')\n",
    "\n",
    "mlp_model = mlp.fit(trainingData)\n",
    "print('Multi-layer Perceptron training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with all the models\n",
    "lr_predictions = lr_model.transform(testData)\n",
    "gbt_predictions = gbt_model.transform(testData)\n",
    "lsvc_predictions = lsvc_model.transform(testData)\n",
    "mlp_predictions = mlp_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Error = 0.0753789\n",
      "Gradient Boosted Test Error = 0.0422353\n",
      "Linear SVC Test Error = 0.0590807\n",
      "Multi-layer Perceptron Test Error = 0.0754032\n"
     ]
    }
   ],
   "source": [
    "# Compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='loan_status', predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "print(\"Logistic Regression Test Error = %g\" % (1.0 - lr_accuracy))\n",
    "\n",
    "gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
    "print(\"Gradient Boosted Test Error = %g\" % (1.0 - gbt_accuracy))\n",
    "\n",
    "lsvc_accuracy = evaluator.evaluate(lsvc_predictions)\n",
    "print(\"Linear SVC Test Error = %g\" % (1.0 - lsvc_accuracy))\n",
    "\n",
    "mlp_accuracy = evaluator.evaluate(mlp_predictions)\n",
    "print(\"Multi-layer Perceptron Test Error = %g\" % (1.0 - mlp_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we stack the models. To do this need to use out-of-fold predictions. Pyspark sucks and I have no idea how to implement this in pyspark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** CREATE EMPTY NP ARRAYS/VECTORS ***\n",
    "\n",
    "*** NEED TO REINTRODUCE AN INDEX COLUMN TO SAVE INDICES ***\n",
    "\n",
    "*** AFTER  RANDOM SPLITTING, GET ALL THE INDICES FROM EACH SPLIT ***\n",
    "\n",
    "*** THIS ALLOWS US TO GENERATE PREDICTIONS BY FILTERING AND STORE THEM APPROPRIATELY ***\n",
    "\n",
    "*** THEN THESE BITCH ASS ARRAYS CAN BE CONCATED AND SENT TO A CSV FOR 2ND LAYER BULLSHIT ***\n",
    "\n",
    "*** THEN WE CONVERT THIS NEW FEATURE MATRIX FROM A CSV BACK INTO A PYSPARK DATAFRAME FOR MODELING ***\n",
    "\n",
    "\n",
    "*** RAW_PREDICTIONS CHOICE IS A VECTOR, NOT SURE HOW TO HANDLE IT, COULD JUST LEAVE IT AS A VECTOR ***\n",
    "\n",
    "*** FEATURE IMPORTANCE WOULD BE NICE AT THE END TOO ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to combine dataframes\n",
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "# Out-of-fold predictions (for stacked models, pure prediction)\n",
    "def get_oof(clf, trainingData, testData, x_test, choice):\n",
    "    training_splits = trainingData.randomSplit([1.0, 1.0, 1.0], 1234)\n",
    "    test_splits = testData.randomSplit([1.0, 1.0, 1.0], 1234)\n",
    "\n",
    "    #oof_train = np.zeros((len(x_train),))\n",
    "    #oof_test = np.zeros((len(x_test),))\n",
    "    #oof_test_skf = np.empty((NFOLDS, len(x_test)))\n",
    "\n",
    "    for i in range(0,len(training_splits)):\n",
    "        #x_tr = x_train.iloc[train_index,:]\n",
    "        #y_tr = y_train[train_index]\n",
    "        #x_te = x_train.iloc[test_index,:]\n",
    "    \n",
    "        #clf.train(x_tr, y_tr)\n",
    "        \n",
    "        model = clf.fit(training_splits[i])\n",
    "        oof_train[test_index] = clf.transform().select(choice)\n",
    "        \n",
    "    \n",
    "        if choice == 'predict':\n",
    "            oof_train[test_index] = clf.predict(x_te)\n",
    "            oof_test_skf[i,:] = clf.predict(x_test)\n",
    "        \n",
    "        elif choice == 'decision_function':\n",
    "            oof_train[test_index] = clf.decision_function(x_te)\n",
    "            oof_test_skf[i,:] = clf.decision_function(x_test)\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)    \n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
