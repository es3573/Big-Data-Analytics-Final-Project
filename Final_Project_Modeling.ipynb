{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is dedicated to Classification models for a preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "#import xgboost as xgb\n",
    "\n",
    "# Going to use these 4 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the preprocessed data into a pandas DataFrame and make sure we only have the features we want (no index column which was created by pd.to_csv automatically). Then we split the preprocessed data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 752938 entries, 0 to 752937\n",
      "Data columns (total 25 columns):\n",
      "term                           752938 non-null int64\n",
      "grade                          752938 non-null int64\n",
      "emp_length                     752938 non-null int64\n",
      "home_ownership                 752938 non-null int64\n",
      "annual_inc                     752938 non-null float64\n",
      "verification_status            752938 non-null int64\n",
      "loan_status                    752938 non-null int64\n",
      "purpose                        752938 non-null int64\n",
      "addr_state                     752938 non-null int64\n",
      "delinq_2yrs                    752938 non-null float64\n",
      "inq_last_6mths                 752938 non-null float64\n",
      "mths_since_last_delinq         752938 non-null float64\n",
      "mths_since_last_record         752938 non-null float64\n",
      "pub_rec                        752938 non-null float64\n",
      "revol_bal                      752938 non-null float64\n",
      "initial_list_status            752938 non-null int64\n",
      "out_prncp                      752938 non-null float64\n",
      "total_rec_int                  752938 non-null float64\n",
      "total_rec_late_fee             752938 non-null float64\n",
      "recoveries                     752938 non-null float64\n",
      "collection_recovery_fee        752938 non-null float64\n",
      "last_pymnt_amnt                752938 non-null float64\n",
      "collections_12_mths_ex_med     752938 non-null float64\n",
      "mths_since_last_major_derog    752938 non-null float64\n",
      "application_type               752938 non-null int64\n",
      "dtypes: float64(15), int64(10)\n",
      "memory usage: 143.6 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/loan_preprocessed_all_hangyu.csv')\n",
    "feature_list = ['term', 'grade', 'emp_length', 'home_ownership', 'annual_inc',\n",
    "       'verification_status', 'loan_status', 'purpose', 'addr_state',\n",
    "       'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_delinq',\n",
    "       'mths_since_last_record', 'pub_rec', 'revol_bal', 'initial_list_status',\n",
    "       'out_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
    "       'collection_recovery_fee', 'last_pymnt_amnt',\n",
    "       'collections_12_mths_ex_med', 'mths_since_last_major_derog',\n",
    "       'application_type']\n",
    "\n",
    "df = df.filter(feature_list)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Split label and features\n",
    "X_train = train_df.drop('loan_status', axis=1)\n",
    "Y_train = train_df['loan_status'].ravel()\n",
    "\n",
    "X_test = test_df.drop('loan_status', axis=1)\n",
    "Y_test = test_df['loan_status'].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the data into test and training sets, we create some helper functions/classes to make the code a bit more managable. This first part will focus only on separate models (no stacking of models) and their confusion matrices/overall accuracy. \n",
    "\n",
    "Some things to consider in general are the effects of balanced data, the number of features, and the correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to extend the Sklearn classifier (just makes it easier to read different models)\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sections serves to manage the parameters for each model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in our parameters for models\n",
    "SEED = 1234\n",
    "\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create and train our models. Then we predict on the test set with our trained models and generate confusion matrices along with its overall accuracy as a metric of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create objects that represent our models\n",
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "rf.train(X_train[0:1000], Y_train[0:1000])\n",
    "ada.train(X_train[0:1000], Y_train[0:1000])\n",
    "gb.train(X_train[0:1000], Y_train[0:1000])\n",
    "svc.train(X_train[0:1000], Y_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set for accuracy metric\n",
    "rf_predictions = rf.predict(X_test)\n",
    "ada_predictions = ada.predict(X_test)\n",
    "gb_predictions = gb.predict(X_test)\n",
    "svc_predictions = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function just computes the confusion matrix and accuracy, then displays both.\n",
    "def accuracy_metric(truth, predictions, model_name):\n",
    "    confusion = pd.crosstab(truth, predictions, rownames = ['Truth'], colnames = ['Prediction'], margins=True)\n",
    "    print('\\n------------------------------\\n')\n",
    "    print(model_name + ' confusion matrix: \\n')\n",
    "    print(confusion)\n",
    "    print('\\nAccuracy: ' + str((confusion[0][0] + confusion[1][1]) / len(truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the accuracies and confusion matrices over all the training data \n",
    "accuracy_metric(Y_test, rf_predictions, 'Random Forest')\n",
    "accuracy_metric(Y_test, ada_predictions, 'AdaBoost')\n",
    "accuracy_metric(Y_test, gb_predictions, 'Gradient Boosting')\n",
    "accuracy_metric(Y_test, svc_predictions, 'Support Vector Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement the stacked modelling approach. This approach simply uses the predictions of a number of models as the training data for another model or layer of models.\n",
    "\n",
    "We need to use Out-of-Fold predictions in order to feed the predictions of the first layer without worrying too much about overfitting the data. At its simplest form, this is K-folding our training data and training/predicting the models on its own fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS = 4 # set this for the number of models we want to stack in the first layer\n",
    "kf = KFold(n_splits= NFOLDS, shuffle = True, random_state=SEED)\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ndimensions = train_df.shape[1]\n",
    "ntest = test_df.shape[0]\n",
    "\n",
    "# Out-of-fold predictions (for stacked models)\n",
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    i = 0\n",
    "    oof_train = np.zeros((len(x_train),))\n",
    "    oof_test = np.zeros((len(x_test),))\n",
    "    oof_test_skf = np.empty((NFOLDS, len(x_test)))\n",
    "\n",
    "    for train_index, test_index in kf.split(x_train):\n",
    "        x_tr = x_train.iloc[train_index,:]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train.iloc[test_index,:]\n",
    "    \n",
    "        clf.train(x_tr, y_tr)\n",
    "    \n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i,:] = clf.predict(x_test)\n",
    "        i += 1\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)    \n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "\n",
    "rf_oof_train, rf_oof_test = get_oof(rf,X_train[0:1000], Y_train[0:1000], X_test) # Random Forest\n",
    "print(\"Random Forest training complete\")\n",
    "\n",
    "ada_oof_train, ada_oof_test = get_oof(ada, X_train[0:1000], Y_train[0:1000], X_test) # AdaBoost \n",
    "print(\" AdaBoost training complete\")\n",
    "\n",
    "gb_oof_train, gb_oof_test = get_oof(gb,X_train[0:1000], Y_train[0:1000], X_test) # Gradient Boost\n",
    "print(\"Gradient Boosting training complete\")\n",
    "\n",
    "svc_oof_train, svc_oof_test = get_oof(svc,X_train[0:1000], Y_train[0:1000], X_test) # Support Vector Classifier\n",
    "print(\"SVC training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new training/test feature sets for layer2 model(s)\n",
    "layer1_train = np.concatenate((rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\n",
    "layer2_test = np.concatenate((rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train layer2 model(s) and predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
