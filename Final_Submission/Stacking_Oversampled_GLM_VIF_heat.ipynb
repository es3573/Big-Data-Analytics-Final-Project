{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell jupyter where pyspark is\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful stuff\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Models and support\n",
    "from pyspark.sql.functions import col, avg\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a SparkSession; \n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Final Project\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed dataset \n",
    "df = spark.read.csv(\"data/oversampled_GLM_VIF_heat.csv\",inferSchema =True,header=True)\n",
    "df = df.drop('_c0')\n",
    "n_features = len(df.columns) - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature vector from the data\n",
    "ignore = ['loan_status']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in df.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "df = assembler.transform(df)\n",
    "df = df.select(['loan_status','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first split into a test and training set\n",
    "(trainingData, testData) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create all our models (Logistic Regression, GBT, Linear SVC, and MLP)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.1, elasticNetParam=0.8,\\\n",
    "                        labelCol = 'loan_status', featuresCol = 'features')\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol='loan_status', featuresCol=\"features\")\n",
    "\n",
    "rf = RandomForestClassifier(numTrees=10,\\\n",
    "                            labelCol=\"loan_status\", featuresCol=\"features\")\n",
    "\n",
    "gbt = GBTClassifier(maxIter=10,\\\n",
    "                    labelCol = 'loan_status', featuresCol = 'features')\n",
    "\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1,\\\n",
    "                 labelCol = 'loan_status', featuresCol = 'features')\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[3, 5, 4, 2], blockSize=128,seed=1234,\\\n",
    "                                     labelCol = 'loan_status', featuresCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to combine dataframes\n",
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "# This is for generating OOF training sets\n",
    "from itertools import combinations\n",
    "\n",
    "# first split training set into k-folds\n",
    "training_splits = trainingData.randomSplit([1.0, 1.0, 1.0], 1234)\n",
    "\n",
    "# generate list of training folds\n",
    "fold_training = []\n",
    "for folds in combinations([2,1,0], 2):\n",
    "    fold_training.append(unionAll(training_splits[folds[0]], training_splits[folds[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOF function that takes the clf and the training_splits list as inputs\n",
    "# and outputs the OOF predictions in a pandas dataframe\n",
    "\n",
    "def oof(clf, fold_training, training_splits, testData):\n",
    "    # fit k-1 folds on 1st stage model and predict on kth fold for all folds\n",
    "    fold_prediction = []\n",
    "    test_prediction = []\n",
    "    for i in range(0, 3):\n",
    "        model = clf.fit(fold_training[i])\n",
    "        fold_prediction.append(model.transform(training_splits[i]).select(['loan_status', 'prediction']).toPandas())\n",
    "        test_prediction.append(model.transform(testData).select('prediction').toPandas())\n",
    "    \n",
    "    # we now have a list of pandas dataframes which we will concat and average (for test set)\n",
    "    layer2_training = pd.concat(fold_prediction)\n",
    "    layer2_test_predictions = pd.concat(test_prediction, axis=1).mean(axis=1)\n",
    "    test_loans = testData.select('loan_status').toPandas()\n",
    "    layer2_test = pd.concat([test_loans, layer2_test_predictions], axis=1)\n",
    "    \n",
    "    return layer2_training, layer2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed models layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_layer2_training, lr_layer2_test = oof(lr, fold_training, training_splits, testData)\n",
    "print('Logistic Regression OOF predictions complete.')\n",
    "\n",
    "gbt_layer2_training, gbt_layer2_test = oof(gbt, fold_training, training_splits, testData)\n",
    "print('Gradient-Boosted OOF predictions complete.')\n",
    "\n",
    "lsvc_layer2_training, lsvc_layer2_test = oof(lsvc, fold_training, training_splits, testData)\n",
    "print('Linear SVC OOF predictions complete.')\n",
    "\n",
    "# concat them all\n",
    "layer2_training = pd.concat([lr_layer2_training, gbt_layer2_training, lsvc_layer2_training], axis=1)\n",
    "layer2_test = pd.concat([lr_layer2_test, gbt_layer2_test, lsvc_layer2_test], axis=1)\n",
    "\n",
    "# send to csv\n",
    "layer2_training.to_csv('data/stacking/layer2_oversampled_training_mixed_new.csv')\n",
    "print('Layer 2 training set csv written.')\n",
    "\n",
    "layer2_test.to_csv('data/stacking/layer2_oversampled_test_mixed_new.csv')\n",
    "print('Layer 2 test set csv written.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree models layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_layer2_training, dt_layer2_test = oof(dt, fold_training, training_splits, testData)\n",
    "print('Decision Tree OOF predictions complete.')\n",
    "\n",
    "rf_layer2_training, rf_layer2_test = oof(rf, fold_training, training_splits, testData)\n",
    "print('Random Forest OOF predictions complete.')\n",
    "\n",
    "#gbt_layer2_training, gbt_layer2_test = oof(gbt, fold_training, training_splits, testData)\n",
    "#print('Gradient-Boosted OOF predictions complete.')\n",
    "\n",
    "# concat them all\n",
    "layer2_training = pd.concat([dt_layer2_training, rf_layer2_training, gbt_layer2_training], axis=1)\n",
    "layer2_test = pd.concat([dt_layer2_test, rf_layer2_test, gbt_layer2_test], axis=1)\n",
    "\n",
    "# send to csv\n",
    "layer2_training.to_csv('data/stacking/layer2_oversampled_training_trees_new.csv')\n",
    "print('Layer 2 trees training set csv written.')\n",
    "\n",
    "layer2_test.to_csv('data/stacking/layer2_oversampled_test_trees_new.csv')\n",
    "print('Layer 2 trees test set csv written.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and reformat data suitable for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# layer2 into pyspark dataframes again (remove index column again and clean names/filter)\n",
    "l2_train_df = spark.read.csv(\"data/stacking/layer2_oversampled_training_mixed_new.csv\",inferSchema =True,header=True)\n",
    "l2_train_df = l2_train_df.drop('_c0')\n",
    "l2_train_df = l2_train_df.select(col('loan_status1').alias('loan_status'), col('prediction2').alias('feature1'),\\\n",
    "                                col('prediction4').alias('feature2'), col('prediction6').alias('feature3'))\n",
    "\n",
    "l2_test_df = spark.read.csv(\"data/stacking/layer2_oversampled_test_mixed_new.csv\",inferSchema =True,header=True)\n",
    "l2_test_df = l2_test_df.drop('_c0')\n",
    "l2_test_df = l2_test_df.select(col('loan_status1').alias('loan_status'), col('02').alias('feature1'),\\\n",
    "                               col('04').alias('feature2'), col('06').alias('feature3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector assembler again\n",
    "ignore = ['loan_status']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in l2_train_df.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "train_df = assembler.transform(l2_train_df)\n",
    "train_df = train_df.select(['loan_status','features'])\n",
    "\n",
    "test_df = assembler.transform(l2_test_df)\n",
    "test_df = test_df.select(['loan_status','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DenseVector since VectorAssembler optimized some entries out\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "ud_f = F.udf(lambda r : Vectors.dense(r),VectorUDT())\n",
    "\n",
    "train_df = train_df.withColumn('features_array',ud_f('features'))\n",
    "train_df = train_df.select('loan_status', col('features_array').alias('features'))\n",
    "\n",
    "test_df = test_df.withColumn('features_array',ud_f('features'))\n",
    "test_df = test_df.select('loan_status', col('features_array').alias('features'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our models and predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-layer Perceptron training complete\n",
      "Mixed Stacked Multi-layer Perceptron Test Error = 0.0639938\n",
      "Mixed Stacked Multi-layer Perceptron F1 Score = 0.935998\n",
      "Mixed Stacked Multi-layer Perceptron AUC Score = 0.936004\n"
     ]
    }
   ],
   "source": [
    "mlp_model = mlp.fit(train_df)\n",
    "print('Multi-layer Perceptron training complete')\n",
    "\n",
    "mlp_predictions = mlp_model.transform(test_df)\n",
    "\n",
    "# Compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='loan_status', predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "mlp_accuracy = evaluator.evaluate(mlp_predictions)\n",
    "print(\"Mixed Stacked Multi-layer Perceptron Test Error = %g\" % (1.0 - mlp_accuracy))\n",
    "\n",
    "# Compute f1 score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='loan_status', predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "mlp_f1 = f1_evaluator.evaluate(mlp_predictions)\n",
    "print(\"Mixed Stacked Multi-layer Perceptron F1 Score = %g\" % mlp_f1)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Compute auc score\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol='loan_status', rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "mlp_auc = auc_evaluator.evaluate(mlp_predictions)\n",
    "print(\"Mixed Stacked Multi-layer Perceptron AUC Score = %g\" % mlp_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the tree based layer1 with the same layer2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer2 into pyspark dataframes again (remove index column again and clean names/filter)\n",
    "l2_train_df = spark.read.csv(\"data/stacking/layer2_oversampled_training_trees_new.csv\",inferSchema =True,header=True)\n",
    "l2_train_df = l2_train_df.drop('_c0')\n",
    "l2_train_df = l2_train_df.select(col('loan_status1').alias('loan_status'), col('prediction2').alias('feature1'),\\\n",
    "                                col('prediction4').alias('feature2'), col('prediction6').alias('feature3'))\n",
    "\n",
    "l2_test_df = spark.read.csv(\"data/stacking/layer2_oversampled_test_trees_new.csv\",inferSchema =True,header=True)\n",
    "l2_test_df = l2_test_df.drop('_c0')\n",
    "l2_test_df = l2_test_df.select(col('loan_status1').alias('loan_status'), col('02').alias('feature1'),\\\n",
    "                               col('04').alias('feature2'), col('06').alias('feature3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector assembler again\n",
    "ignore = ['loan_status']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in l2_train_df.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "train_df = assembler.transform(l2_train_df)\n",
    "train_df = train_df.select(['loan_status','features'])\n",
    "\n",
    "test_df = assembler.transform(l2_test_df)\n",
    "test_df = test_df.select(['loan_status','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DenseVector since VectorAssembler optimized some entries out\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "ud_f = F.udf(lambda r : Vectors.dense(r),VectorUDT())\n",
    "\n",
    "train_df = train_df.withColumn('features_array',ud_f('features'))\n",
    "train_df = train_df.select('loan_status', col('features_array').alias('features'))\n",
    "\n",
    "test_df = test_df.withColumn('features_array',ud_f('features'))\n",
    "test_df = test_df.select('loan_status', col('features_array').alias('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-layer Perceptron training complete\n",
      "Tree Stacked Multi-layer Perceptron Layer 2 Test Error = 0.057365\n",
      "Tree Stacked Multi-layer Perceptron F1 Score = 0.942635\n",
      "Tree Stacked Multi-layer Perceptron AUC Score = 0.942635\n"
     ]
    }
   ],
   "source": [
    "mlp_model = mlp.fit(train_df)\n",
    "print('Multi-layer Perceptron training complete')\n",
    "\n",
    "mlp_predictions = mlp_model.transform(test_df)\n",
    "\n",
    "# Compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='loan_status', predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "mlp_accuracy = evaluator.evaluate(mlp_predictions)\n",
    "print(\"Tree Stacked Multi-layer Perceptron Layer 2 Test Error = %g\" % (1.0 - mlp_accuracy))\n",
    "\n",
    "# Compute f1 score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='loan_status', predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "mlp_f1 = f1_evaluator.evaluate(mlp_predictions)\n",
    "print(\"Tree Stacked Multi-layer Perceptron F1 Score = %g\" % mlp_f1)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Compute auc score\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol='loan_status', rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "mlp_auc = auc_evaluator.evaluate(mlp_predictions)\n",
    "print(\"Tree Stacked Multi-layer Perceptron AUC Score = %g\" % mlp_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
